# 图卷积
## API
### **scipy.sparse**: 根据坐标和对应的数字生成稀疏矩阵
```python
import scipy.sparse as sp
adj = sp.coo_matrix((np.ones(4), ([0,2,3,1],[1,2,3,2] )), shape=(4,4), dtype=np.float32)  
print(adj)
print(adj.dense())   # 
```
> (0, 1)	1.0
> 
> (2, 2)	1.0
> 
> (3, 3)	1.0
> 
> (1, 2)	1.0

> [[0. 1. 0. 0.]
> 
> [0. 0. 1. 0.]
> 
> [0. 0. 1. 0.]
> 
> [0. 0. 0. 1.]]


### **spmm**:稀疏乘法
```python
import torch  
a = torch.sparse.FloatTensor(torch.LongTensor([[0,2,3,1], [1,2,3,2]]), torch.ones(4), (4,4))  
b = torch.randint(0, 2, (4,4), dtype=torch.float32)  
c = torch.spmm(a, b)  
print(torch.spmm(a.to_dense(), b) == torch.spmm(a, b))  
print(c)

```

    tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
		
    tensor([[0., 1., 1., 0.],
        [1., 0., 0., 1.],
        [1., 0., 0., 1.],
        [1., 0., 1., 1.]])
		
```python
import scipy.sparse as sp  
import torch  
import numpy as np  
import torch.nn as nn  
import torch.nn.functional as F  
from torch.autograd import Variable  
  
"""NETWORK"""  
###########################################################################
class GraphAttentionLayer(nn.Module):  
    """Simple GAT layer, similar to https://arxiv.org/abs/1710.10903"""  
    def __init__(self, in_features, out_features, dropout, alpha, concat=True):  
        super(GraphAttentionLayer, self).__init__()  
        self.dropout = dropout  
        self.in_features = in_features  
        self.out_features = out_features  
        self.alpha = alpha  
        self.concat = concat  
        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))  
        nn.init.xavier_uniform_(self.W.data, gain=1.414)  
        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))  
        nn.init.xavier_uniform_(self.a.data, gain=1.414)  
        self.leakyrelu = nn.LeakyReLU(self.alpha)  
  
  
    def forward(self, h, adj):  
        Wh = torch.mm(h, self.W)  # 矩阵乘法，只能用于二维  
        e = self._prepare_attentional_mechanism_input(Wh)  
  
        zero_vec = -9e-15 * torch.ones_like(e)  
        attention = torch.where(adj > 0, e, zero_vec)  
        attention = F.softmax(attention, dim=1)  
        attention = F.dropout(attention, self.dropout, training=self.training)  
        h_prime = torch.matmul(attention, Wh)  # 比torch.mm高级，能用做高维  
        if self.concat:  
            return F.elu(h_prime)  
        else:  
            return h_prime  
  
  
    def _prepare_attentional_mechanism_input(self, Wh):  
        # Wh.shape (N, out_feature)  
        # self.a.shape (2 * out_feature, 1) 
		# Wh1&2.shape (N, 1)    
		# e.shape (N, N)     
		Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])  
        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])  
        e = Wh1 + Wh2  
        return self.leakyrelu(e)  
  
    def __repr__(self):  
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'  
  
class GAT(nn.Module):  
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):  
        """Dense version of GAT."""  
        super(GAT, self).__init__()  
        self.dropout = dropout  
  
        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]  
        for i, attention in enumerate(self.attentions):  
            self.add_module('attention_{}'.format(i), attention)  
  
        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)  
  
    def forward(self, x, adj):  
        x = F.dropout(x, self.dropout, training=self.training)  
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)  
        x = F.dropout(x, self.dropout, training=self.training)  
        x = F.elu(self.out_att(x, adj))  
        return F.log_softmax(x, dim=1)  
  
###########################################################################
  
def normalize_features(features):  
    rowsum = np.array(features.sum(1))  # 对每一个特征点求和  
    r_inv = np.power(rowsum, -1).flatten() # 特征和的倒数  
    r_inv[np.isinf(r_inv)] = 0.  # 防止0的倒数  
    r_mat_inv = sp.diags(r_inv)  # 构建对角矩阵  
    features = r_mat_inv.dot(features)  
    return features  
  
def normalize_adj(adj):  
    rowsum = np.array(adj.sum(1))  
    r_inv_sqrt = np.power(rowsum, -0.5).flatten()  
    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.  
    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)  
    return adj.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)  
  
def sparse_mx2torch_sparse_tensor(sparse_mx):  
    """Convert a scipy sparse matrix to a torch sparse tensor."""  
    sparse_mx = sparse_mx.tocoo().astype(np.float32)  
    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))  
    values = torch.from_numpy(sparse_mx.data)  
    shape = torch.Size(sparse_mx.shape)  
    return torch.sparse.FloatTensor(indices, values, shape)  
  
if __name__ == '__main__':  
  
    # 假设节点个数为2708,类别数为7，每个节点的特征维度为100  
    node_num = 2708  
    cls_num = 7  
    features_num = 100  
    edges = np.random.randint(low=0, high=node_num, size=(3000, 2))   # edges表示两节点的边的连接情况  
    labels = np.random.randint(low=0, high=7, size=(node_num, 1))   # 所有节点的分类标签  
    features =np.random.random((node_num, features_num))  
    # labels_onehot = torch.zeros(node_num, cls_num).scatter_(1, torch.from_numpy(labels), 1).numpy()  
  
    # 根据边的信息构建单向稀疏邻接矩阵  
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)  
  
    # 构建对称邻接矩阵  
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)  
  
    # 邻接矩阵归一化,并转为tensor  
    adj = normalize_adj(adj + sp.eye(adj.shape[0]))  
  
    # 特征归一化  
    features = normalize_features(features)  
  
    model = GAT(nfeat=features.shape[1],  
                nhid=20,  
                nclass=cls_num,  
                dropout=0.2,  
                nheads=8,  
                alpha=0.2)  
  
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  
  
    # 数据转为tensor  
    #adj = sparse_mx2torch_sparse_tensor(adj).cuda()  
	adj = torch.FloatTensor(np.array(adj.todense())).cuda()  
    features = torch.FloatTensor(features).cuda()  
    labels = torch.LongTensor(labels).cuda()  
  
  
    features, adj, labels = Variable(features), Variable(adj), Variable(labels.squeeze())  
  
    model.cuda().train()  
    for epoch in range(1000):  
        optimizer.zero_grad()  
        output = model(features, adj)  
  
        loss = F.nll_loss(output, labels)  
        loss.backward()  
        optimizer.step()  
        print(loss)
```
