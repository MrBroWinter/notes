## anchor
```python
import torch  
import numpy as np  
import torch.nn as nn  
  
class Anchors(nn.Module):  
    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):  
        super(Anchors, self).__init__()  
        self.pyramid_levels = pyramid_levels  
        self.strides = strides  
        self.sizes = sizes  
        self.ratios = np.array(ratios)  
        self.scales = np.array(scales)  
  
    def forward(self, image):  
        device = image.device  
  
        image_shape = np.array(image.shape[2:])  
        level_shapes = [(image_shape + 2**x - 1) // (2**x) for x in self.pyramid_levels]  
  
        # compute anchors over all pyramid lebels  
        all_anchors = np.zeros((0,4)).astype(np.float32)  
  
        for idx, p in enumerate(self.pyramid_levels):  
            base_anchor = self.generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)  
            anchors = self.shift(level_shapes[idx], self.strides[idx], base_anchor)  
            all_anchors = np.append(all_anchors, anchors, axis=0)  
  
  
        all_anchors = np.expand_dims(all_anchors, axis=0)  
        return torch.from_numpy(all_anchors.astype(np.float32)).to(device)  
  
    def generate_anchors(self, base_size=16, ratios=None, scales=None):  
        """  
        Generate anchor (reference) windows by enumerating aspect ratios X        scales w.r.t. a reference window.        """        if ratios == None:  
            ratios = np.array([0.5, 1, 2])  
  
        if scales == None:  
            scales = np.array([2**0, 2**(1.0 / 3.0), 2**(2.0 / 3.0)])  
  
        num_anchors = len(ratios) * len(scales)  
  
        anchor = np.zeros((num_anchors, 4))  # 后两位为宽高信息  
        anchor[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T  
  
        # 计算面积  
        areas = anchor[:, 2] * anchor[:, 3]  
  
        # correct for ratios      # 没理解这个做法  
        anchor[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))  
        anchor[:, 3] = anchor[:, 2] * np.repeat(ratios, len(scales))  
  
        #  transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2),中心点坐标（0， 0）  
        anchor[:, 0::2] -= np.tile(anchor[:, 2] * 0.5, (2, 1)).T  
        anchor[:, 1::2] -= np.tile(anchor[:, 3] * 0.5, (2, 1)).T  
  
        return anchor  
  
    def shift(self, shape, stride, anchor):  
        """  
        shape: 特征大小  
        stride：特征的一个单位大小对应原始图像的大小  
        anchor：计算出的base anchor  
        """        shift_x = (np.arange(0, shape[1]) + 0.5) * stride  
        shift_y = (np.arange(0, shape[0]) + 0.5) * stride  
  
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)  
        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()  
  
        # add A anchors (1, A, 4) to  
        # cell K shifts (K, 1, 4) to get        # shift anchors (K, A, 4)        # reshape to (K*A, 4) shifted anchors        A = anchor.shape[0]  # base anchor的个数：9  
        K = shifts.shape[0]  # 一张图片在该尺度上shift的次数  
  
        all_anchor = (anchor + np.expand_dims(shifts, axis=1))  
        all_anchor = all_anchor.squeeze().reshape(K*A, 4)  
        return all_anchor  
  
  
  
if __name__ == '__main__':  
    image = torch.randn(1, 1, 512, 512)  
    anchors = Anchors(pyramid_levels=[3,4,5],  
                      strides=[2**x for x in [3,4,5]],  
                      sizes=[54, 72, 98],  
                      ratios=None,  
                      scales=None)  
    out = anchors(image)  
    print(out.shape)
```

## IOU boardcast
```python
import numpy as np  
  
def iou(bbox, gt):# [xmin,ymin,xmax,ymax]  
    left_top = np.maximum(bbox[:, None, :2], gt[:, :2])  
    right_bottom = np.minimum(bbox[:, None, 2:], gt[:, 2:])  
    wh = np.maximum(right_bottom - left_top, 0)  
    intersection_area = wh[:, :, 0] * wh[:, :, 1]  
    bbox_area = (bbox[:,2] - bbox[:, 0]) * (bbox[:,3] - bbox[:, 1])  
    gt_area = (gt[:,2] - gt[:, 0]) * (gt[:,3] - gt[:, 1])  
    iou = intersection_area / (np.expand_dims(bbox_area, axis=1) + gt_area - intersection_area + 1e-4)  
  
    return iou  
  
if __name__ == '__main__':  
    bbox = np.array([[5,5,15,15], [7,10, 12,15]])  
    gt = np.array([[5,6,16,16], [8,11,12,15]])  
    res = iou(bbox, gt)
```


## 检测损失
```python
import torch  
import torch.nn as nn  
  
def calc_iou(a, b):  
    area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])  
    area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])  
  
    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], dim=1), b[:, 0])  
    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], dim=1), b[:, 1])  
  
    iw = torch.clamp(iw, min=0)  
    ih = torch.clamp(ih, min=0)  
  
    intersection_area = iw * ih  
  
    iou = intersection_area / (torch.unsqueeze(area_a, dim=1) + area_b - intersection_area + 1e-4)  
  
class Focal_Loss(nn.Module):  
    def __init__(self, alpha=0.25, gamma=2.0, pos_thresh=0.5, neg_thresh=0.4):  
        super(Focal_Loss, self).__init__()  
        self.alpha = alpha  
        self.gamma = gamma  
        self.pos_thresh = pos_thresh  
        self.neg_thresh = neg_thresh  
        self.bce_loss = nn.BCEWithLogitsLoss(reduce=False)  
  
    def forward(self, classifications, regressions, anchors, annotations,debug_info=None):  
        """  
        classifications: [b, num_anchors, num_classes]        regressions: [b, num_anchors, 4]        anchors: [1, num_anchors, 4]        annotations: [b, tg_num, 5]        """        device = classifications.device  
        batch_size = classifications.shape[0]  
        classification_losses = []  
        regression_losses = []  
        anchor = anchors[0, :, :]  
  
        anchor_width = anchor[:, 2] - anchor[:, 0]  
        anchor_height = anchor[:, 3] - anchor[:, 1]  
  
        anchor_center_x = anchor[:, 0] + anchor_width / 2  
        anchor_center_y = anchor[:, 1] + anchor_height / 2  
  
        for j in range(batch_size):  
            classification = classifications[j, :, :]  
            regression = regressions[j, :, :]  
            bbox_annotation = annotations[j, :, :]   # 此处的annotation有好多无效值，需要筛选  
            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]  
            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)  # 防止出现0和1，否则后面loss中的log会出现无穷大  
  
            # 没有目标的情况计算损失  
            if bbox_annotation.shape[0] == 0:  
                alpha_factor = torch.ones(classification.shape, device=device) * self.alpha  
                alpha_factor = 1.0 - alpha_factor  
                focal_weight = classification  
                focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma)  
  
                bce = -(torch.log(1.0 - classification))  
                cls_loss = focal_weight * bce  
                classification_losses.append(cls_loss.sum())  
                regression_losses.append(torch.tensor(0, device=device).float())  
  
                continue  
  
            IoU = calc_iou(anchor, bbox_annotation[:, :4])  
  
            Iou_max, Iou_argmax = torch.max(IoU, dim=1)  # num_anchors * 1, 每个anchor对应最大的预测框  
  
            targets = torch.ones(classification.shape, device=device) * -1  
            targets[torch.lt(Iou_max, self.neg_thresh), :] = 0  
            positive_indices = torch.ge(Iou_max, self.pos_thresh)  
  
            num_positive_anchors = positive_indices.sum()  
  
            assigned_annotations = bbox_annotation[Iou_argmax, :]   # 每一个anchor是对哪一个gt负责的  
  
            targets[positive_indices, :] = 0  
            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1  
  
            alpha_factor = torch.ones(targets.shape, device = device) * self.alpha  
            alpha_factor = torch.where(torch.eq(targets, 1.0), alpha_factor, 1.0 - alpha_factor)  
            focal_weight = torch.where(torch.eq(targets, 1.0), 1.0 - classification, classification)  
            focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma)  
  
            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))  
  
            cls_loss = focal_weight * bce  
            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape, device=device))  
  
            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.float(), min=1.0))  
  
  
            # 计算回归损失  
            if positive_indices.sum() > 0:  
                assigned_annotations = assigned_annotations[positive_indices, :]  
                anchor_widths_pi = anchor_width[positive_indices]  
                anchor_heights_pi = anchor_height[positive_indices]  
                anchor_center_x_pi = anchor_center_x[positive_indices]  
                anchor_center_y_pi = anchor_center_y[positive_indices]  
  
                gt_width = assigned_annotations[:, 2] - assigned_annotations[:, 0]  
                gt_height = assigned_annotations[:, 3] - assigned_annotations[:, 1]  
                gt_center_x = assigned_annotations[:, 0] + 0.5 * gt_width  
                gt_center_y = assigned_annotations[:, 1] + 0.5 * gt_height  
  
                gt_width = torch.clamp(gt_width, min=1)  
                gt_height = torch.clamp(gt_height, min=1)  
  
                targets_dx = (gt_center_x - anchor_center_x_pi) / anchor_widths_pi  
                targets_dy = (gt_center_y - anchor_center_y_pi) / anchor_heights_pi  
                targets_dw = torch.log(gt_width / anchor_widths_pi)  
                targets_dh = torch.log(gt_height / anchor_heights_pi)  
  
                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))  
                targets = targets.t()  
  
                targets = targets / torch.tensor([[0.1, 0.1, 0.2, 0.2]], device=device)  
                regression_diff = torch.abs(targets - regression[positive_indices, :])  
                  
                # smooth损失  
                regression_loss = torch.where(  
                    torch.le(regression_diff, 1.0 / 9.0),  
                    0.5 * 9.0 * torch.pow(regression_diff, 2),  
                    regression_diff - 0.5 / 9.0,  
                )  
                regression_losses.append(regression_loss.mean())  
            else:  
                regression_losses.append(torch.tensor(0, device=device).float())  
  
        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(  
            dim=0, keepdim=True)
```
